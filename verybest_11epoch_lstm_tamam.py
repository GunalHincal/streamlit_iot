# -*- coding: utf-8 -*-
"""VERYBEST_LSTM_TAMAM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fOelCcRBhH3JIBjAXCNC4JoXyhsrnHEN
"""

# Drive ile Colab notebook bağlama
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
pd.set_option('display.max_columns', None)  # bütün sütunları göster
pd.set_option('display.max_rows', None)     # bütün satırları göster
pd.set_option('display.width', 500)  # sütunlar max 500 tane gösterilsin
pd.set_option('display.expand_frame_repr', False)

df = pd.read_excel("/content/drive/MyDrive/COLAB/DA_Proje/capped_hour_data_iot.xlsx")

# Varyans katsayısını hesaplama
coeff_variance = {}
for column in df.columns:
    if column != "time":
        mean = df[column].mean()
        std = df[column].std()
        coeff_variance[column] = (std / mean) * 100

coeff_variance

df.max()

import pandas as pd
import seaborn as sns
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense, Dropout
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler



"""# Datayı Eğitim için Hazırlama"""

#Separate dates for future plotting
train_dates = pd.to_datetime(df['time'])

#Variables for training
cols = list(df)[1:7]
cols

df_for_training = df[cols]
cols

"""# Standartlaştırma"""

#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized
# normalize the dataset
scaler = StandardScaler()
scaler = scaler.fit(df_for_training)
df_for_training_scaled = scaler.transform(df_for_training)

# MinMaxScaler: Verilen 2 değer arasında değişken dönüşümü, yaygınca kullanılan yöntemlerden birisidir
# Verilen 2 değer arasında değişken dönüşümü, yaygınca kullanılan yöntemlerden birisidir
# özellikle 0-1, 0-10 arası 1-5 arası gibi dönüştürmek istediğimiz özel aralıklar varsa bu durumda bu MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaler = scaler.fit(df_for_training)
df_for_training_scaled = scaler.transform(df_for_training)

# RobustScaler: Medyanı çıkar iqr'a böl.
# Neden robust, Standart Scaler ortalamayı bütün gözlem birimlerinden çıkarıp std ye böler,  standart sapma da ortalama da veri setinde
# aykırı olan metrikleriden etkilenen metriklerdir. Bu nedenle bu sefer bütün değerlerden medyanı çıkartsak daha sonra bölme işlemi için std gibi aykırı değerlerden etkilenen bi değere değil de
# iqr a bölsek daha iyi olabilir çünkü hem merkezi eğilimi ve değişimi göz önünde bulundurmuş oluruz hem de daha robust yani güçlü bir standartlaştırma yapmış oluruz
scaler = RobustScaler()
scaler = scaler.fit(df_for_training)
df_for_training_scaled = scaler.transform(df_for_training)

df_for_training.shape

"""# Model Kurma"""

#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features.
#In this example, the n_features is 1. We will make timesteps = 48 (past hours data used for training).

#Empty lists to be populated using formatted training data
trainX = []
trainY = []

n_future = 1   # Number of days we want to look into the future based on the past hours.
n_past = 48  # Number of past hours we want to use to predict the future.

#Reformat input data into a shape: (n_samples x timesteps x n_features)
#In my example, my df_for_training_scaled has a shape (9281, 6)
#9281 refers to the number of data points and 6 refers to the columns (multi-variables).
for i in range(n_past, len(df_for_training_scaled) - n_future +1):
    trainX.append(df_for_training_scaled[i - n_past:i, 0:df_for_training.shape[1]])
    trainY.append(df_for_training_scaled[i + n_future - 1:i + n_future, 0])

trainX, trainY = np.array(trainX), np.array(trainY)

print('trainX shape == {}.'.format(trainX.shape))
print('trainY shape == {}.'.format(trainY.shape))

#In my case, trainX has a shape (9233, 48, 6).
#9233 because we are looking back 48 hours (9281 - 48 = 9233).
#Remember that we cannot look back 48 hours until we get to the 1th hour.
#Also, trainY has a shape (9233, 1). Our model only predicts a single value, but it needs multiple variables (6 in my example) to make this prediction.
#This is why we can only predict a single hour after our training, the hour after where our data ends.
#To predict more hours in future, we need all the 6 variables which we do not have.
#We need to predict all variables if we want to do that.

model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))
model.add(LSTM(32, activation='relu', return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(trainY.shape[1]))

from keras import backend as K
# RMSE as a custom metric
def root_mean_squared_error(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=[root_mean_squared_error])
model.summary()

# fit the model
history = model.fit(trainX, trainY, epochs=11, batch_size=16, validation_split=0.1, verbose=1, shuffle=False)

history.history.keys()

history.history.values()

#plt.figure(figsize=(10,6))
plt.plot(history.history['loss'], label='Training loss', color='blue')
plt.plot(history.history['root_mean_squared_error'], label='RMSE', color='black')
plt.plot(history.history['val_loss'], label='Validation loss', color='red')
#plt.plot(history.history['val_root_mean_squared_error'], label='Val_RMSE', color='green')
plt.title('Eğitim ve Doğrulama Kaybı')
#plt.ylim(0, 1)  # Burada y-ekseni sınırlarını ayarlıyoruz
#plt.xlim(0, 5)  # Burada x-ekseni sınırlarını ayarlıyoruz
plt.legend()
plt.show()

"""# Modeli Kaydetme ve Geri Yükleme"""

# Modeli HDF5 dosya formatında kaydetme
model.save('11epoch_iot_model.h5')

# Modeli colab klasörüne kaydetme
model.save('/content/drive/MyDrive/COLAB/DA_Proje/iot saved models/11epoch_iot_model.h5')

# Modeli çalışma ortamına geri yükleme
from keras.models import load_model
import keras.backend as K

# Define the root_mean_squared_error metric
def root_mean_squared_error(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true)))

# Load the model with the custom metric
loaded_model = load_model('/content/drive/MyDrive/COLAB/DA_Proje/iot saved models/verybest_iot_model.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})

"""# Future Prediction"""

# Lets forecast now
# n_future = 24 saat * 7 gün
# n_future = 168

# 1 aylık tahmin de yapalım 30 x 24
n_future = 720

forecast_period_dates = pd.date_range(list(train_dates)[-1], periods=n_future, freq="H").tolist()

# Make prediction
forecast = model.predict(trainX[-n_future:])

len(forecast)

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_vibx = scaler.inverse_transform(forecast_copies)[:,0]

actualVibxFor168 = df_for_training["vibx"][-n_future:]
predictedVibxFor168 = y_pred_future_vibx

#y_pred_future_vibx

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_vibz = scaler.inverse_transform(forecast_copies)[:,1]

actualVibzFor168 = df_for_training["vibz"][-n_future:]
predictedVibzFor168 = y_pred_future_vibz

#y_pred_future_vibz

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_temp = scaler.inverse_transform(forecast_copies)[:,2]

actualTempFor168 = df_for_training["temp"][-n_future:]
predictedTempFor168 = y_pred_future_temp

#y_pred_future_temp

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_zacc = scaler.inverse_transform(forecast_copies)[:,3]

actualZaccFor168 = df_for_training["zacc"][-n_future:]
predictedZaccFor168 = y_pred_future_zacc

#y_pred_future_zacc

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_zfreq = scaler.inverse_transform(forecast_copies)[:,4]

actualZfreqFor168 = df_for_training["zfreq"][-n_future:]
predictedZfreqFor168 = y_pred_future_zfreq

#y_pred_future_zfreq

forecast_copies = np.repeat(forecast, df_for_training.shape[1], axis=-1)
y_pred_future_xkurt = scaler.inverse_transform(forecast_copies)[:,5]

actualXkurtFor168 = df_for_training["xkurt"][-n_future:]
predictedXkurtFor168 = y_pred_future_xkurt

#y_pred_future_xkurt

# dataframe e bakmak için array formatını pddataframe ile çevirdik
forecast_copies= pd.DataFrame(data=forecast_copies)
forecast_copies.head()









"""# Burayı Kullanma"""

#Remember that we can only predict one day in future as our model needs 5 variables as inputs for prediction.
#We only have all 5 variables until the last day in our dataset.
n_past = 50
n_hours_for_prediction = 49  #let us predict past 15 days

predict_period_dates = pd.date_range(list(train_dates)[-n_past], periods=n_hours_for_prediction, freq="H").tolist()
print(predict_period_dates)

#Make prediction
prediction = model.predict(trainX[-n_hours_for_prediction:]) #shape = (n, 1) where n is the n_days_for_prediction

#Perform inverse transformation to rescale back to original range
#Since we used 5 variables for transform, the inverse expects same dimensions
#Therefore, let us copy our values 5 times and discard them after inverse transform
prediction_copies = np.repeat(prediction, df_for_training.shape[1], axis=-1)
y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]

# Convert timestamp to date
forecast_dates = []
for time_i in predict_period_dates:
    forecast_dates.append(time_i.date())

df_forecast = pd.DataFrame({'time':np.array(forecast_dates), 'vibx':y_pred_future})
df_forecast['time']=pd.to_datetime(df_forecast['time'])

original = df[['time', 'vibx']]
original['time']=pd.to_datetime(original['time'])
original = original.loc[original['time'] >= '2021-01-22']

sns.lineplot(original['time'], original['vibx'])
sns.lineplot(df_forecast['time'], df_forecast['vibx'])

"""# Tahmin Başarısı Grafikleri"""

def plotForecast(x, y1, y2, xlabel= "TIME", ylabel="VALUES"):
    fig, ax = plt.subplots(figsize=(8,4))
    #fig.subplots_adjust(bottom=0.15, left=0.2)
    plt.plot(x, y1, label="ACTUAL", color="red")
    plt.plot(x, y2, label="PREDICTION", color="black")
    # Eksen ve başlık ayarları
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.xticks(rotation=45, ha='right')
    plt.title("Forecast 1 month")
    #plt.title("Forecast one week")
    plt.legend()
    plt.show()

plotForecast(forecast_period_dates, actualVibxFor168, predictedVibxFor168, "VIBX Forecast")

plotForecast(forecast_period_dates, actualVibzFor168, predictedVibzFor168, "VIBZ Forecast")

plotForecast(forecast_period_dates, actualTempFor168, predictedTempFor168, "Temperature Forecast")

plotForecast(forecast_period_dates, actualZaccFor168, predictedZaccFor168, "ZACC Forecast")

plotForecast(forecast_period_dates, actualZfreqFor168, predictedZfreqFor168, "ZFREQ Forecast")

plotForecast(forecast_period_dates, actualXkurtFor168, predictedXkurtFor168, "XKURT Forecast")



"""# Hata Yüzdeleri (MAPE)"""

from sklearn.metrics import mean_squared_error as mse
from math import sqrt

def mse_rmse_calculate(actual, predict):
    mean_square_error = mse(actual,predict)
    root_mean_square_error = sqrt(mean_square_error)
    print(root_mean_square_error)

mse_rmse_calculate(actualVibxFor168, predictedVibxFor168)
# 0.265
#
# 0.2635683289688892
#0.2606577454540019

mse_rmse_calculate(actualVibzFor168,predictedVibzFor168)
# 0.227
#
# 0.23764962447335894
# 0.21695713102530534

mse_rmse_calculate(actualTempFor168,predictedTempFor168)
# 5.281
#
# 5.224508322128979
# 5.346895467440828

mse_rmse_calculate(actualZaccFor168,predictedZaccFor168)
# 0.128
#
# 0.13495098865647812
# 0.12360566571025858

mse_rmse_calculate(actualZfreqFor168,predictedZfreqFor168)
# 0.026
#
# 0.0271858561626714
# 0.025065019754166445

mse_rmse_calculate(actualXkurtFor168, predictedXkurtFor168)
# 0.265
#
# 0.2635683289688892
# 0.2662526858058813

from sklearn.metrics import mean_absolute_error as mae

def mae_calculate(actual, predict):
    mean_absolute_error = mae(actual,predict)
    print(mean_absolute_error)
    return mean_absolute_error

vibx_mae = mae_calculate(actualVibxFor168,predictedVibxFor168)
# 0.16
#
# 0.16779453028725747
# 0.16367184014639055
# 0.14230968584406223

vibz_mae = mae_calculate(actualVibzFor168,predictedVibzFor168)
# 0.19
#
# 0.19799448590557314
# 0.19428029425623075
# 0.17429795730261236

temp_mae = mae_calculate(actualTempFor168,predictedTempFor168)
# 4.73
#
# 4.621092414312087
# 4.592854400276996
4.767071136475852

zacc_mae = mae_calculate(actualZaccFor168,predictedZaccFor168)
# 0.10
#
# 0.09998285670903563
# 0.09856883449510766
# 0.08966519051511099

zfreq_mae = mae_calculate(actualZfreqFor168,predictedZfreqFor168)
# 0.02
#
# 0.022169246179904404
# 0.021655713613333596
# 0.01969949840778558

xkurt_mae = mae_calculate(actualXkurtFor168,predictedXkurtFor168)
# 0.20
#
# 0.19911915889617116
# 0.1990394538242812
# 0.18121540383590984

# 15-10-11 epoch

vibx_mean = df.vibx.mean()

ratio_vibx = (vibx_mae / vibx_mean) * 100
ratio_vibx

# 10.95
# 10.323164502406044
# 9.547583430425485

vibz_mean = df.vibz.mean()

ratio_vibz = (vibz_mae / vibz_mean) * 100
ratio_vibz

# 20.14
# 19.50826124189061
# 18.258080730872724

temp_mean = df.temp.mean()

ratio_temp = (temp_mae / temp_mean) * 100
ratio_temp

# 12.90
# 12.791802347208606
# 12.986629096266434

zacc_mean = df.zacc.mean()

ratio_zacc = (zacc_mae / zacc_mean) * 100
ratio_zacc

# 20.71
# 18.323267718879233
# 17.65292364821024

zfreq_mean = df.zfreq.mean()

ratio_zfreq = (zfreq_mae / zfreq_mean) * 100
ratio_zfreq

# 22.68
# 20.659585672065237
# 19.696632883760525

xkurt_mean = df.xkurt.mean()

ratio_xkurt = (xkurt_mae / xkurt_mean) * 100
ratio_xkurt

# 5.54
# 5.12177517955606
# 4.968231186520866